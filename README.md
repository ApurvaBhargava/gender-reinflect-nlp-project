# NLP with Representation Learning Project:

## Brainstorming:

We discussed multiple project ideas and we're currently faced with the following questions:

1) Should we focus on one language and explore various societal biases (gender, race, religions, etc.)?

2) Or should we focus on multiple languages (3 languages maybe?) and explore a specific kind of bias?

3) What are the NLP applications that we could potentially explore (MT, coreference resolution, embeddings, etc.)?

To answer these questions, we would need to do a literature review on what's out there in terms of data, modeling, and evaluation metrics. So here are some papers I know of on debiasing NLP systems across multiple areas: 

### Pretrained Language models:

[StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/pdf/2004.09456.pdf)

[CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models](https://arxiv.org/abs/2010.00133)


### Machine Translation:

[Evaluating Gender Bias in Machine Translation](https://www.aclweb.org/anthology/P19-1164.pdf): [Video](https://vimeo.com/384485671)

[Automatically Identifying Gender Issues in Machine Translation using Perturbations](https://arxiv.org/pdf/2004.14065.pdf)

[Automatic Gender Identification and Reinflection in Arabic](https://www.aclweb.org/anthology/W19-3822.pdf)

[Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection](https://www.aclweb.org/anthology/W19-3807.pdf)

[Getting Gender Right in Neural Machine Translation](https://www.aclweb.org/anthology/D18-1334.pdf)

[Equalizing Gender Bias in Neural Machine Translation with Word Embeddings Techniques](https://arxiv.org/pdf/1901.03116.pdf)

### Coreference Resolution:

[Gender Bias in Coreference Resolution](https://www.aclweb.org/anthology/N18-2002.pdf)

[Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods](https://arxiv.org/pdf/1804.06876.pdf)

[BERT Masked Language Modeling for Co-reference Resolution](https://www.aclweb.org/anthology/W19-3811.pdf)


### Word Embeddings:

[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf)

[Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://arxiv.org/pdf/1903.03862.pdf)

[Measuring Bias in Contextualized Word Representations](https://www.aclweb.org/anthology/W19-3823.pdf)

[Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://arxiv.org/pdf/1904.04047.pdf)

[Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer](https://arxiv.org/pdf/2005.00699.pdf)

[Learning Gender-Neutral Word Embeddings](https://www.aclweb.org/anthology/D18-1521.pdf)

### Data Augmentation Techniques:

[Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology](https://arxiv.org/pdf/1906.04571.pdf)

[Gender Bias in Neural Natural Language Processing](https://arxiv.org/pdf/1807.11714.pdf)

[Itâ€™s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution](https://www.aclweb.org/anthology/D19-1530.pdf)
